import time
from pymilvus import connections, Collection, FieldSchema, CollectionSchema, DataType, utility

# --- Configuration ---
SOURCE_CONFIG = {
    "uri": "localhost:19530",
    "db_name": "default",
    "collection_name": "original_collection"
}

TARGET_CONFIG = {
    "uri": "localhost:19530",
    "db_name": "backup_db", # Ensure this DB exists or use "default"
    "collection_name": "restored_collection"
}

def duplicate_collection():
    # 1. Connect to Source
    connections.connect(alias="source", uri=SOURCE_CONFIG["uri"], db_name=SOURCE_CONFIG["db_name"])
    
    if not utility.has_collection(SOURCE_CONFIG["collection_name"], using="source"):
        print(f"Source collection {SOURCE_CONFIG['collection_name']} not found!")
        return

    src_col = Collection(SOURCE_CONFIG["collection_name"], using="source")
    src_col.load() # Load to ensure we can read data

    # 2. Extract Schema and Index Info
    schema = src_col.schema
    index_details = [idx.to_dict() for idx in src_col.indexes]
    
    # 3. Connect to Target
    connections.connect(alias="target", uri=TARGET_CONFIG["uri"], db_name=TARGET_CONFIG["db_name"])
    
    # Remove target if it already exists (Optional)
    if utility.has_collection(TARGET_CONFIG["collection_name"], using="target"):
        utility.drop_collection(TARGET_CONFIG["collection_name"], using="target")

    # 4. Create New Collection with same schema
    dest_col = Collection(
        name=TARGET_CONFIG["collection_name"],
        schema=schema,
        using="target"
    )
    print(f"Created collection: {TARGET_CONFIG['collection_name']}")

    # 5. Migrate Data (Batching is recommended for large datasets)
    # Using query with empty expression to get all data
    results = src_col.query(expr="", output_fields=["*"])
    if results:
        dest_col.insert(results)
        dest_col.flush()
        print(f"Successfully migrated {len(results)} entities.")

    # 6. Recreate Indexes
    for idx in index_details:
        field_name = idx['field_name']
        index_params = idx['index_param']
        dest_col.create_index(field_name=field_name, index_params=index_params)
        print(f"Index recreated on field: {field_name}")

    # 7. Load and Verify
    dest_col.load()
    print("Collection loaded. Ready for search.")
    
    # Simple Search Test (Assumes your vector field is named 'embeddings')
    # search_params = {"metric_type": "L2", "params": {"nprobe": 10}}
    # results = dest_col.search(data=[[0.1] * dim], anns_field="embeddings", param=search_params, limit=1)
    
if __name__ == "__main__":
    duplicate_collection()
